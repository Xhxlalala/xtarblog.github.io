<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>论文阅读 on Xtar&#39;s blog</title>
    <link>https://xtar.netlify.app/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</link>
    <description>Recent content in 论文阅读 on Xtar&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 05 Oct 2022 21:27:16 +0800</lastBuildDate><atom:link href="https://xtar.netlify.app/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>论文阅读15:《Defending Label Inference and Backdoor Attacks in Vertical Federated Learning》</title>
      <link>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB15/</link>
      <pubDate>Wed, 05 Oct 2022 21:27:16 +0800</pubDate>
      
      <guid>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB15/</guid>
      <description>1 介绍 本文主要介绍了纵向联邦学习( VFL )下的标签推理攻击和后门攻击，并提出了一种全新的防御方法。 纵向联邦学习下攻击和防御尤其具有挑战，因为无法</description>
    </item>
    
    <item>
      <title>论文阅读14:《TransMIA：Membership Inference Attacks Using Transfer Shadow Training》</title>
      <link>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB14/</link>
      <pubDate>Wed, 05 Oct 2022 21:09:31 +0800</pubDate>
      
      <guid>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB14/</guid>
      <description>1 介绍 迁移学习近些年来很受欢迎。在这篇文章中，作者提出了 TransMIA ，使用transfer learning 在源模型上进行成员推理攻击当敌手只能访问迁移模型的参数时。</description>
    </item>
    
    <item>
      <title>论文阅读13:《EncoderMI：Membership Inference against Pre-trainedEncoders in Contrastive Learning》</title>
      <link>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB13/</link>
      <pubDate>Sun, 25 Sep 2022 21:12:44 +0800</pubDate>
      
      <guid>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB13/</guid>
      <description>1 介绍 给定一个无标签的图片集或者(图片，文本)对集，对比学习试着去预训练一个图片编码器，作为众多下游任务的特征提取器。给定一个图片编码器，下</description>
    </item>
    
    <item>
      <title>论文阅读12:《Membership Inference Attacks Against Recommender Systems》</title>
      <link>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB12/</link>
      <pubDate>Sun, 25 Sep 2022 20:32:27 +0800</pubDate>
      
      <guid>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB12/</guid>
      <description>1 介绍 近年来，推荐系统取得了良好的表现，成为应用最广泛的网络应用之一。然而，推荐系统通常使用高度敏感的用户数据进行训练，从而推荐系统潜在的数</description>
    </item>
    
    <item>
      <title>论文阅读11:《Enhanced Mixup Training：a Defense Method Against Membership InferenceAttack》</title>
      <link>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB11/</link>
      <pubDate>Sun, 18 Sep 2022 17:07:41 +0800</pubDate>
      
      <guid>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB11/</guid>
      <description>1 介绍 MIA 通常利用 model-based 或者 metric-based 推理方法来推测一个具体的数据样本是否是目标模型训练集中的成员。现有的防御措施主要有以下两个缺陷： 当前的防御措施主要用</description>
    </item>
    
    <item>
      <title>论文阅读10:《MemGuard：Defending against Black-Box Membership Inference Attacks via Adversarial Examples》</title>
      <link>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB10/</link>
      <pubDate>Sun, 18 Sep 2022 16:42:36 +0800</pubDate>
      
      <guid>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB10/</guid>
      <description>1 介绍 成员推理攻击中，攻击者通常训练一个二分类器，输入样本的置信值向量，输出该样本是否为训练集中的成员。成员推理攻击主要利用的是目标模型的过</description>
    </item>
    
    <item>
      <title>论文阅读9:《Property Inference Attacks Against GANs》</title>
      <link>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB9/</link>
      <pubDate>Sun, 04 Sep 2022 21:43:52 +0800</pubDate>
      
      <guid>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB9/</guid>
      <description>1 介绍 本文提出了第一个针对GANs的 属性推理攻击。 具体来说，敌手试着去推测宏观的训练数据集属性，例如用来训练GAN模型的某一属性的样本分布。</description>
    </item>
    
    <item>
      <title>论文阅读8:《 Property Inference Attacks on Fully Connected Neural Networks using Permutation Invariant Representations》</title>
      <link>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB8/</link>
      <pubDate>Sun, 04 Sep 2022 21:19:19 +0800</pubDate>
      
      <guid>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB8/</guid>
      <description>1 介绍 本文主要介绍了针对全连接神经网络（FCNNs）的 白盒 环境下的属性推理攻击。具体来说就是推理训练数据集的全局属性，例如数据被生产的环境或</description>
    </item>
    
    <item>
      <title>论文阅读7:《Label-Only Membership Inference Attacks》</title>
      <link>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB7/</link>
      <pubDate>Sun, 28 Aug 2022 20:45:42 +0800</pubDate>
      
      <guid>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB7/</guid>
      <description>1 介绍 以往的成员推理攻击利用的是模型在训练集和测试集上置信值的不同。如果敌手只能获得标签而无法获得具体的置信值，这种攻击就无法得到应用了。本</description>
    </item>
    
    <item>
      <title>论文阅读6:《Leakage of Dataset Properties in Multi-Party Machine Learning》</title>
      <link>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB6/</link>
      <pubDate>Sun, 28 Aug 2022 20:06:39 +0800</pubDate>
      
      <guid>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB6/</guid>
      <description>1 介绍 本文主要介绍了针对中心化多参与方机器学习环境下数据集属性的推理攻击，主要是 population-level 的数据集属性推理。安全多方机器学习提供参与方黑盒的访问以便</description>
    </item>
    
    <item>
      <title>论文阅读5:《GAN-Leaks: A Taxonomy of Membership Inference Attacks against Generative Models》</title>
      <link>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB5/</link>
      <pubDate>Mon, 15 Aug 2022 16:22:14 +0800</pubDate>
      
      <guid>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB5/</guid>
      <description>1. 文章概述 首次对针对生成模型的成员推理攻击进行了分类 提供了一种有理论根据的攻击校准技术 2. 背景知识 2.1 Generative Model： 生成模型主要有两种，分别是：</description>
    </item>
    
    <item>
      <title>论文阅读4:《Exploiting Unintended Feature Leakage in Collaborative Learning》</title>
      <link>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB4/</link>
      <pubDate>Tue, 02 Aug 2022 18:07:15 +0800</pubDate>
      
      <guid>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB4/</guid>
      <description>1. 文章概述 本文阐释了 Collaborative learning 的参数更新过程存在无意地泄露用户训练数据信息的漏洞，并且本文采用了消极和积极的两种攻击方式来利用这个漏洞。首先，利用</description>
    </item>
    
    <item>
      <title>论文阅读3:《Inference Attack and Defense on the Distributed Private Fair Machine Learning Framework》</title>
      <link>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB3/</link>
      <pubDate>Sat, 23 Jul 2022 21:43:46 +0800</pubDate>
      
      <guid>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB3/</guid>
      <description>1. 介绍 公平和隐私都是机器学习中重要的社会规范。在本文作者19年的一篇文章中，作者提出了一种分布式框架来学习公平的预测模型同时保护用户的人口统</description>
    </item>
    
    <item>
      <title>论文阅读2:《Membership Inference Attack on Graph Neural Networks》</title>
      <link>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB2/</link>
      <pubDate>Sun, 17 Jul 2022 21:45:27 +0800</pubDate>
      
      <guid>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB2/</guid>
      <description>1. 介绍 图神经网络（GNNs） 通常用来生成传统的的深度神经网络或图数据，使用此神经网络可以处理节点分类、链接预测或图分类等任务。本文主要关注训</description>
    </item>
    
    <item>
      <title>论文阅读1:《Membership Inference Attacks Against Machine Learning Models》</title>
      <link>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</link>
      <pubDate>Tue, 05 Jul 2022 11:34:47 +0800</pubDate>
      
      <guid>https://xtar.netlify.app/post/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</guid>
      <description>1. 介绍 本文主要介绍了一种机器学习模型的推理攻击 &amp;mdash; 成员推理攻击。成员推理攻击即：给定一个数据记录以及对模型的黑盒访问，推断该数据记录是否在模型</description>
    </item>
    
  </channel>
</rss>
